{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reinforcement Learning with TF-Agents API with CarMaker as an Environment\n",
    "This notebook is used to train and evaluate an agent in a CarMaker environment.\n",
    "\n",
    "First of all, necessary libaries have to be imported:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# If GPU should not be used uncomment following line.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logzero\n",
    "from logzero import logger as logging\n",
    "\n",
    "from tf_agents.environments import py_environment, batched_py_environment, tf_py_environment\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.experimental.train.utils import spec_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from CM_py_env import CarMakerEnv\n",
    "from server import Server"
   ]
  },
  {
   "source": [
    "If all imports were succesful, a main directory has to be defined. This directory will contain all selected metrics saved in a tensor file, which can be viewed using tensorboard. \n",
    "\n",
    "Start tensorboard using following command: `tensorboard --logdir path_of_main_directory_or_root_dir`\n",
    "\n",
    "Uncomment first 3 lines and comment line 4 to automatically choose a folder name and create a folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#root_dir = \"%s/rl_data/run%s\" % (os.getcwd(),  timestr)\n",
    "#os.mkdir(root_dir)\n",
    "root_dir = \"/home/andreas-z97x-ud3h/CM_Projects/CM_RL_Driver/RL/rl_data/first_trial_1\""
   ]
  },
  {
   "source": [
    "## Hyperparameters\n",
    "Important hyperparameters of the SAC algorithm can be changed in the following cell:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100000 # number of steps\n",
    "\n",
    "initial_collect_steps = 10000 # number of transitions to fill the replay buffer\n",
    "train_steps_per_iteration = 1 # Train steps for each transition\n",
    "replay_buffer_capacity = 400000 # number of transitions saved inside the replay buffer\n",
    "collect_actor_num_steps = 1 # Number of transitions before a train step is made. Must be 'None' if following parameter is given.\n",
    "collect_actor_num_episodes = None # Number of episodes before a train step is made.\n",
    "\n",
    "batch_size = 512 # Number of samples which are choosen randomly inside the replay buffer.\n",
    "\n",
    "critic_learning_rate = 4e-4 # Learning rate of the critic network\n",
    "actor_learning_rate = 4e-4 # Learning rate of the actor network\n",
    "alpha_learning_rate = 4e-4 # Update rate of the entropy regularization\n",
    "target_update_tau = 0.005 # Not changed\n",
    "target_update_period = 1 # Not changed\n",
    "gamma = 0.998 # Discount factor\n",
    "reward_scale_factor = 100.0 # Reward scaling\n",
    "\n",
    "actor_fc_layer_params = (256, 256) # 2 layers with 256 units each\n",
    "critic_joint_fc_layer_params = (256, 256) # 2 layers with 256 units each\n",
    "\n",
    "log_interval = 5000 # Log interval of TensorFlow metrics after a number of steps\n",
    "\n",
    "num_eval_episodes = 16 # Number of evalutation episodes done at each evaluation\n",
    "eval_interval = 100000 # Number of steps after a evaluation is done\n",
    "\n",
    "policy_save_interval = 10000 # Saving the policy after a number of steps to make evaluation possible after interruption or end of training.\n",
    "\n",
    "summary_interval=1000 # ?\n",
    "summaries_flush_secs=10 # ?\n",
    "\n",
    "eval_metrics_callback=None # Not used\n",
    "\n",
    "# Save intervals for networks and replay buffer to resume training after interruption or end of python kernel at last checkpoint.\n",
    "train_checkpoint_interval=5000\n",
    "policy_checkpoint_interval=5000\n",
    "rb_checkpoint_interval=5000"
   ]
  },
  {
   "source": [
    "## Logging and summary/metric file writer\n",
    "\n",
    "Create a log-file:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logzero.logfile(\"%s/rotating-logfile.log\" % root_dir, maxBytes=1e6, backupCount=3)\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "source": [
    "Create summary/metric file writer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(root_dir, 'train')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "train_summary_writer.set_as_default()\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]"
   ]
  },
  {
   "source": [
    "## Create Environments:\n",
    "\n",
    "Multiple environments can be created and summarized to one batched environment:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Environment for collecting data during eval, and one for eval\n",
    "#Collect langsamer\n",
    "\n",
    "collect_env = CarMakerEnv(RTFac=999999, mode='collect', gamma=gamma, server=0)\n",
    "#collect_env2 = CarMakerEnv(RTFac=2, mode='collect', gamma=gamma, server=1)\n",
    "#collect_env3 = CarMakerEnv(RTFac=1, mode='collect', gamma=gamma, server=2)\n",
    "#collect_env4 = CarMakerEnv(RTFac=1, mode='collect', gamma=gamma, server=3)\n",
    "\n",
    "#collect_env = batched_py_environment.BatchedPyEnvironment( [collect_env1, collect_env2] )\n",
    "#eval_env = CarMakerEnv(RTFac=999999, mode='evaluate', gamma=gamma, server=1)\n",
    "\n",
    "#eval_env = collect_env\n",
    "\n",
    "tf_collect_env = tf_py_environment.TFPyEnvironment(collect_env)\n",
    "tf_eval_env = tf_collect_env\n"
   ]
  },
  {
   "source": [
    "Cell to check the observation and action specs:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\nTensorSpec(shape=(22,), dtype=tf.float32, name='observation')\nAction Spec:\nBoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array([-1.  , -9.42], dtype=float32), maximum=array([1.  , 9.42], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(tf_collect_env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(tf_collect_env.action_spec())"
   ]
  },
  {
   "source": [
    "## Agents \n",
    "Critic and actor network are created. The same configuration as in the TensorFlow SAC example is used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(tf_collect_env))\n",
    "\n",
    "\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "      (observation_spec, action_spec),\n",
    "      observation_fc_layer_params=None,\n",
    "      action_fc_layer_params=None,\n",
    "      joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "      kernel_initializer='glorot_uniform',\n",
    "      last_kernel_initializer='glorot_uniform')\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=actor_fc_layer_params,\n",
    "    continuous_projection_net=(\n",
    "        tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "source": [
    "Create the SAC agent using the critic_net and the actor_net. Also, create or load a global step tensor "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init agent\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "tf_agent = sac_agent.SacAgent(\n",
    "    time_step_spec,\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    train_step_counter=global_step)\n",
    "\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "source": [
    "## Replay buffer\n",
    "Initialize an empty replay buffer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "def _filter_invalid_transition(trajectories, unused_arg1):\n",
    "      return ~trajectories.is_boundary()[0]\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size, num_steps=2, num_parallel_calls=2).prefetch(50)\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "source": [
    "## Policies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policies\n",
    "# Agent has 2 policies\n",
    "\n",
    "# agent.policy — The main policy that is used for evaluation and deployment.\n",
    "# agent.collect_policy — A second policy that is used for data collection.\n",
    "\n",
    "tf_eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n",
    "\n",
    "tf_collect_policy = tf_agent.collect_policy\n",
    "\n",
    "# Additional random policy for initial collector\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "  tf_collect_env.time_step_spec(), tf_collect_env.action_spec())\n",
    "  "
   ]
  },
  {
   "source": [
    "## Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_collect_env.batch_size),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_collect_env.batch_size),\n",
    "]"
   ]
  },
  {
   "source": [
    "## Checkpointer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f15ada159a0>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=train_dir,\n",
    "    agent=tf_agent,\n",
    "    global_step=global_step,\n",
    "    metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "policy_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(train_dir, 'policy'),\n",
    "    policy=tf_eval_policy,\n",
    "    global_step=global_step)\n",
    "rb_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n",
    "    max_to_keep=1,\n",
    "    replay_buffer=replay_buffer)\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "rb_checkpointer.initialize_or_restore()"
   ]
  },
  {
   "source": [
    "## Driver: Random Policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor with the random policy and collect experiences to seed the replay buffer with\n",
    "\n",
    "initial_collect_actor = dynamic_step_driver.DynamicStepDriver(\n",
    "  tf_eval_env,\n",
    "  random_policy,\n",
    "  num_steps=1,\n",
    "  observers=replay_observer+train_metrics)"
   ]
  },
  {
   "source": [
    "## Driver: Collect actor\n",
    "\n",
    "Gathers experience during training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Actor with the collect policy to gather more experiences during training\n",
    "\n",
    "# Steps per run nun 5 statt 1, weil smoother?\n",
    "\n",
    "\n",
    "if collect_actor_num_episodes is not None and collect_actor_num_steps is not None:\n",
    "  raise ValueError(\"Define num episodes OR num steps. One of them must be None.\")\n",
    "elif collect_actor_num_episodes is None:\n",
    "  collect_actor = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_collect_env,\n",
    "    tf_collect_policy,\n",
    "    num_steps=collect_actor_num_steps,\n",
    "    observers=replay_observer+train_metrics)\n",
    "elif collect_actor_num_steps is None:\n",
    "  collect_actor = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_collect_env,\n",
    "    tf_collect_policy,\n",
    "    num_episodes=collect_actor_num_episodes,\n",
    "    observers=replay_observer+train_metrics)\n",
    "  "
   ]
  },
  {
   "source": [
    "## TensorFlow Graph:\n",
    "\n",
    "Is not necessary. This was used in order to examine an potential performance gain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_actor.run = common.function(initial_collect_actor.run)\n",
    "collect_actor.run = common.function(collect_actor.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "source": [
    "## Get initial state, if resumed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = None\n",
    "policy_state = tf_collect_policy.get_initial_state(tf_collect_env.batch_size)\n",
    "timed_at_step = global_step.numpy()\n",
    "time_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_val = train_metrics[0].result()\n",
    "Server(14100).send_gui(\"DVAWrite RL_Agent.Episodes %d\" % episode_val)\n",
    "global_step_val = global_step.numpy()"
   ]
  },
  {
   "source": [
    "## Random samples for replay buffer\n",
    "Fill replay buffer with random samples (using the random policy)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10000/10000 [01:03<00:00, 156.83it/s]\n"
     ]
    }
   ],
   "source": [
    "if replay_buffer.num_frames() == 0:\n",
    "    Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 2)\n",
    "    # Collect initial replay data.\n",
    "    for _ in tqdm(range(initial_collect_steps)):\n",
    "        initial_collect_actor.run()\n",
    "        if episode_val != train_metrics[0].result().numpy():\n",
    "            episode_val = train_metrics[0].result().numpy()\n",
    "            Server(14100).send_gui(\"DVAWrite RL_Agent.Episodes %d\" % episode_val)\n",
    "    Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    experience, _ = next(iterator)\n",
    "    return tf_agent.train(experience)\n",
    "\n",
    "train_step = common.function(train_step)"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 210320 19:06:56 <ipython-input-21-6a927e3ad22b>:17] step = 5000, loss = 28114.072266\n",
      "[I 210320 19:06:56 <ipython-input-21-6a927e3ad22b>:19] 28.416 steps/sec\n",
      "[I 210320 19:09:48 <ipython-input-21-6a927e3ad22b>:17] step = 10000, loss = 45054.640625\n",
      "[I 210320 19:09:48 <ipython-input-21-6a927e3ad22b>:19] 31.294 steps/sec\n",
      "[I 210320 19:12:42 <ipython-input-21-6a927e3ad22b>:17] step = 15000, loss = 51741.468750\n",
      "[I 210320 19:12:42 <ipython-input-21-6a927e3ad22b>:19] 30.708 steps/sec\n",
      "[I 210320 19:15:24 <ipython-input-21-6a927e3ad22b>:17] step = 20000, loss = 66775.531250\n",
      "[I 210320 19:15:24 <ipython-input-21-6a927e3ad22b>:19] 33.368 steps/sec\n",
      "[I 210320 19:18:01 <ipython-input-21-6a927e3ad22b>:17] step = 25000, loss = 90924.742188\n",
      "[I 210320 19:18:01 <ipython-input-21-6a927e3ad22b>:19] 34.408 steps/sec\n",
      "[I 210320 19:20:39 <ipython-input-21-6a927e3ad22b>:17] step = 30000, loss = 85948.890625\n",
      "[I 210320 19:20:39 <ipython-input-21-6a927e3ad22b>:19] 34.132 steps/sec\n",
      "[I 210320 19:23:09 <ipython-input-21-6a927e3ad22b>:17] step = 35000, loss = 125563.000000\n",
      "[I 210320 19:23:09 <ipython-input-21-6a927e3ad22b>:19] 36.212 steps/sec\n",
      "[I 210320 19:25:41 <ipython-input-21-6a927e3ad22b>:17] step = 40000, loss = 89337.554688\n",
      "[I 210320 19:25:41 <ipython-input-21-6a927e3ad22b>:19] 35.519 steps/sec\n",
      "[I 210320 19:28:09 <ipython-input-21-6a927e3ad22b>:17] step = 45000, loss = 99043.695312\n",
      "[I 210320 19:28:09 <ipython-input-21-6a927e3ad22b>:19] 36.544 steps/sec\n",
      "[I 210320 19:30:55 <ipython-input-21-6a927e3ad22b>:17] step = 50000, loss = 116492.851562\n",
      "[I 210320 19:30:55 <ipython-input-21-6a927e3ad22b>:19] 32.384 steps/sec\n",
      "[I 210320 19:33:33 <ipython-input-21-6a927e3ad22b>:17] step = 55000, loss = 114148.179688\n",
      "[I 210320 19:33:33 <ipython-input-21-6a927e3ad22b>:19] 34.325 steps/sec\n",
      "[I 210320 19:36:10 <ipython-input-21-6a927e3ad22b>:17] step = 60000, loss = 104523.218750\n",
      "[I 210320 19:36:10 <ipython-input-21-6a927e3ad22b>:19] 34.235 steps/sec\n",
      "[I 210320 19:38:55 <ipython-input-21-6a927e3ad22b>:17] step = 65000, loss = 172068.578125\n",
      "[I 210320 19:38:55 <ipython-input-21-6a927e3ad22b>:19] 32.596 steps/sec\n",
      "[I 210320 19:41:27 <ipython-input-21-6a927e3ad22b>:17] step = 70000, loss = 134361.218750\n",
      "[I 210320 19:41:27 <ipython-input-21-6a927e3ad22b>:19] 35.686 steps/sec\n",
      "[I 210320 19:44:05 <ipython-input-21-6a927e3ad22b>:17] step = 75000, loss = 71608.031250\n",
      "[I 210320 19:44:05 <ipython-input-21-6a927e3ad22b>:19] 34.062 steps/sec\n",
      "[I 210320 19:46:33 <ipython-input-21-6a927e3ad22b>:17] step = 80000, loss = 119783.914062\n",
      "[I 210320 19:46:33 <ipython-input-21-6a927e3ad22b>:19] 36.736 steps/sec\n",
      "[I 210320 19:49:07 <ipython-input-21-6a927e3ad22b>:17] step = 85000, loss = 144600.578125\n",
      "[I 210320 19:49:08 <ipython-input-21-6a927e3ad22b>:19] 34.957 steps/sec\n",
      "[I 210320 19:51:34 <ipython-input-21-6a927e3ad22b>:17] step = 90000, loss = 240369.250000\n",
      "[I 210320 19:51:34 <ipython-input-21-6a927e3ad22b>:19] 36.976 steps/sec\n",
      "[I 210320 19:54:03 <ipython-input-21-6a927e3ad22b>:17] step = 95000, loss = 172095.890625\n",
      "[I 210320 19:54:03 <ipython-input-21-6a927e3ad22b>:19] 36.413 steps/sec\n",
      "[I 210320 19:56:29 <ipython-input-21-6a927e3ad22b>:17] step = 100000, loss = 92642.023438\n",
      "[I 210320 19:56:29 <ipython-input-21-6a927e3ad22b>:19] 37.213 steps/sec\n"
     ]
    }
   ],
   "source": [
    "while global_step_val < num_iterations:\n",
    "    start_time = time.time()\n",
    "    time_step, policy_state = collect_actor.run(\n",
    "        time_step=time_step,\n",
    "        policy_state=policy_state,\n",
    "    )\n",
    "    for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "    time_acc += time.time() - start_time\n",
    "\n",
    "    global_step_val = global_step.numpy()\n",
    "    if episode_val != train_metrics[0].result().numpy():\n",
    "        episode_val = train_metrics[0].result().numpy()\n",
    "        Server(14100).send_gui(\"DVAWrite RL_Agent.Episodes %d\" % episode_val)\n",
    "\n",
    "    if global_step_val % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step_val, train_loss.loss)\n",
    "        steps_per_sec = (global_step_val - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step_val\n",
    "        time_acc = 0\n",
    "\n",
    "    for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "    if global_step_val % eval_interval == 0:\n",
    "        Server(14100).send_gui(\"StopSim\")\n",
    "        time.sleep(1)\n",
    "        Server(14100).send_gui(\"StartSim\")\n",
    "        time.sleep(0.25)\n",
    "        Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 1)\n",
    "        Server(14100).send_gui(\"SetSimTimeAcc %d\" % 50)\n",
    "        time.sleep(0.25)\n",
    "        tf_eval_env.reset()\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            tf_eval_env,\n",
    "            tf_eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        tf_eval_env.reset()\n",
    "        #time.sleep(0.5)\n",
    "        Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 0)\n",
    "        Server(14100).send_gui(\"SetSimTimeAcc %d\" % 6)\n",
    "\n",
    "    if eval_metrics_callback is not None:\n",
    "        eval_metrics_callback(results, global_step_val)\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    if global_step_val % train_checkpoint_interval == 0:\n",
    "        train_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "    if global_step_val % policy_checkpoint_interval == 0:\n",
    "        policy_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "    if global_step_val % rb_checkpoint_interval == 0:\n",
    "        rb_checkpointer.save(global_step=global_step_val)"
   ]
  },
  {
   "source": [
    "## Evaluation of final Policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = metric_utils.eager_compute(\n",
    "    eval_metrics,\n",
    "    tf_eval_env,\n",
    "    tf_eval_policy,\n",
    "    num_episodes=num_eval_episodes,\n",
    "    train_step=global_step,\n",
    "    summary_writer=eval_summary_writer,\n",
    "    summary_prefix='Metrics',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
