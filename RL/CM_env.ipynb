{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import time\n",
    "\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "#import reverb\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logzero\n",
    "from logzero import logger as logging\n",
    "\n",
    "from tf_agents.environments import py_environment, batched_py_environment, tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.experimental.train import actor\n",
    "from tf_agents.experimental.train import learner\n",
    "from tf_agents.experimental.train import triggers\n",
    "from tf_agents.experimental.train.utils import spec_utils\n",
    "from tf_agents.experimental.train.utils import strategy_utils\n",
    "from tf_agents.experimental.train.utils import train_utils\n",
    "from tf_agents.metrics import py_metrics, tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy, py_tf_policy\n",
    "from tf_agents.policies import random_py_policy, random_tf_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.eval import metric_utils\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "\n",
    "\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from CM_py_env import CarMakerEnv\n",
    "from server import Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#root_dir = \"%s/rl_data/run%s\" % (os.getcwd(),  timestr)\n",
    "#os.mkdir(root_dir)\n",
    "root_dir = \"/home/andreas-z97x-ud3h/CM_Projects/CM_RL_Driver/RL/rl_data/run20201228-teleport-rfac_0_998\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter\n",
    "num_iterations = 4000000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
    "train_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 400000 # @param {type:\"integer\"}\n",
    "collect_actor_num_steps = 2\n",
    "collect_actor_num_episodes = None # steps or episodes must be none\n",
    "\n",
    "batch_size = 512 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 4e-4 # @param {typ\"}ber\"}\n",
    "actor_learning_rate = 8e-4 # @param {type\"}er\"}\n",
    "alpha_learning_rate = 4e-4 # @param {type\"}er\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.998 # @param {type:\"number\"}\n",
    "reward_scale_factor = 3.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 16 # @param {type:\"integer\"}\n",
    "eval_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "summary_interval=1000\n",
    "summaries_flush_secs=10\n",
    "\n",
    "eval_metrics_callback=None\n",
    "\n",
    "train_checkpoint_interval=5000\n",
    "policy_checkpoint_interval=5000\n",
    "rb_checkpoint_interval=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logzero.logfile(\"%s/rotating-logfile.log\" % root_dir, maxBytes=1e6, backupCount=3)\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(root_dir, 'train')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "train_summary_writer.set_as_default()\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Environment for collecting data during eval, and one for eval\n",
    "#Collect langsamer\n",
    "\n",
    "collect_env = CarMakerEnv(RTFac=999999, mode='collect', gamma=gamma, server=0)\n",
    "#collect_env2 = CarMakerEnv(RTFac=2, mode='collect', gamma=gamma, server=1)\n",
    "#collect_env3 = CarMakerEnv(RTFac=1, mode='collect', gamma=gamma, server=2)\n",
    "#collect_env4 = CarMakerEnv(RTFac=1, mode='collect', gamma=gamma, server=3)\n",
    "\n",
    "#collect_env = batched_py_environment.BatchedPyEnvironment( [collect_env1, collect_env2] )\n",
    "#eval_env = CarMakerEnv(RTFac=999999, mode='evaluate', gamma=gamma, server=1)\n",
    "\n",
    "#eval_env = collect_env\n",
    "\n",
    "tf_collect_env = tf_py_environment.TFPyEnvironment(collect_env)\n",
    "tf_eval_env = tf_collect_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\nTensorSpec(shape=(23,), dtype=tf.float32, name='observation')\nAction Spec:\nBoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array([-1.   , -4.999], dtype=float32), maximum=array([1.   , 4.999], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Check specs\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(tf_collect_env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(tf_collect_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agents\n",
    "# All variables and Agents need to be created under strategy.scope(), as you'll see below.\n",
    "\n",
    "# Critic for estimate of action values\n",
    "# Input is observation and an action\n",
    "# Output is estimate of action value (to see how good it would be)\n",
    "\n",
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(tf_collect_env))\n",
    "\n",
    "\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "      (observation_spec, action_spec),\n",
    "      observation_fc_layer_params=None,\n",
    "      action_fc_layer_params=None,\n",
    "      joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "      kernel_initializer='glorot_uniform',\n",
    "      last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor network predicts parameters for a tanh-squashed MultivariateNormalDiag distribution.\n",
    "# Will be sampled conditioned on the current state (observation), if generation of action is needed.\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=actor_fc_layer_params,\n",
    "    continuous_projection_net=(\n",
    "        tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init agent\n",
    "5\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "tf_agent = sac_agent.SacAgent(\n",
    "    time_step_spec,\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    train_step_counter=global_step)\n",
    "\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=replay_buffer_capacity)\n",
    "# 6 instead of 2 for TD(5)\n",
    "\n",
    "def _filter_invalid_transition(trajectories, unused_arg1):\n",
    "      return ~trajectories.is_boundary()[0]\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size, num_steps=2, num_parallel_calls=2).unbatch().filter(\n",
    "            _filter_invalid_transition).batch(batch_size).prefetch(100)\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policies\n",
    "# Agent has 2 policies\n",
    "\n",
    "# agent.policy — The main policy that is used for evaluation and deployment.\n",
    "# agent.collect_policy — A second policy that is used for data collection.\n",
    "\n",
    "tf_eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n",
    "\n",
    "tf_collect_policy = tf_agent.collect_policy\n",
    "\n",
    "# Additional random policy for initial collector\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "  tf_collect_env.time_step_spec(), tf_collect_env.action_spec())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_collect_env.batch_size),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_collect_env.batch_size),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f8be49b7280>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=train_dir,\n",
    "    agent=tf_agent,\n",
    "    global_step=global_step,\n",
    "    metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "policy_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(train_dir, 'policy'),\n",
    "    policy=tf_eval_policy,\n",
    "    global_step=global_step)\n",
    "rb_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n",
    "    max_to_keep=1,\n",
    "    replay_buffer=replay_buffer)\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "rb_checkpointer.initialize_or_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor with the random policy and collect experiences to seed the replay buffer with\n",
    "\n",
    "initial_collect_actor = dynamic_step_driver.DynamicStepDriver(\n",
    "  tf_eval_env,\n",
    "  random_policy,\n",
    "  num_steps=1,\n",
    "  observers=replay_observer+train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Actor with the collect policy to gather more experiences during training\n",
    "\n",
    "# Steps per run nun 5 statt 1, weil smoother?\n",
    "\n",
    "\n",
    "if collect_actor_num_episodes is not None and collect_actor_num_steps is not None:\n",
    "  raise ValueError(\"Define num episodes OR num steps. One of them must be None.\")\n",
    "elif collect_actor_num_episodes is None:\n",
    "  collect_actor = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_collect_env,\n",
    "    tf_collect_policy,\n",
    "    num_steps=collect_actor_num_steps,\n",
    "    observers=replay_observer+train_metrics)\n",
    "elif collect_actor_num_steps is None:\n",
    "  collect_actor = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_collect_env,\n",
    "    tf_collect_policy,\n",
    "    num_episodes=collect_actor_num_episodes,\n",
    "    observers=replay_observer+train_metrics)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_actor.run = common.function(initial_collect_actor.run)\n",
    "collect_actor.run = common.function(collect_actor.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = None\n",
    "policy_state = tf_collect_policy.get_initial_state(tf_collect_env.batch_size)\n",
    "timed_at_step = global_step.numpy()\n",
    "time_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_val = train_metrics[0].result()\n",
    "Server(14100).send_gui(\"DVAWrite RL_Agent.Episodes %d\" % episode_val)\n",
    "global_step_val = global_step.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10000/10000 [01:06<00:00, 150.93it/s]\n"
     ]
    }
   ],
   "source": [
    "if replay_buffer.num_frames() == 0:\n",
    "    Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 1)\n",
    "    # Collect initial replay data.\n",
    "    for _ in tqdm(range(initial_collect_steps)):\n",
    "        initial_collect_actor.run()\n",
    "        if episode_val != train_metrics[0].result().numpy():\n",
    "            episode_val = train_metrics[0].result().numpy()\n",
    "            Server(14100).send_gui(\"DVAWrite RL_Agent.Episodes %d\" % episode_val)\n",
    "    Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    experience, _ = next(iterator)\n",
    "    return tf_agent.train(experience)\n",
    "\n",
    "train_step = common.function(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 201229 00:01:25 <ipython-input-22-8f25392378e3>:17] step = 10100, loss = 1079029.250000\n",
      "[I 201229 00:01:25 <ipython-input-22-8f25392378e3>:19] 6.720 steps/sec\n",
      "[I 201229 00:01:57 <ipython-input-22-8f25392378e3>:17] step = 10200, loss = 789142.875000\n",
      "[I 201229 00:01:57 <ipython-input-22-8f25392378e3>:19] 3.125 steps/sec\n",
      "[I 201229 00:02:03 <ipython-input-22-8f25392378e3>:17] step = 10300, loss = 808301.312500\n",
      "[I 201229 00:02:03 <ipython-input-22-8f25392378e3>:19] 18.008 steps/sec\n",
      "[I 201229 00:02:09 <ipython-input-22-8f25392378e3>:17] step = 10400, loss = 1577099.500000\n",
      "[I 201229 00:02:09 <ipython-input-22-8f25392378e3>:19] 17.962 steps/sec\n",
      "[I 201229 00:02:16 <ipython-input-22-8f25392378e3>:17] step = 10500, loss = 723690.000000\n",
      "[I 201229 00:02:16 <ipython-input-22-8f25392378e3>:19] 15.371 steps/sec\n",
      "[I 201229 00:02:23 <ipython-input-22-8f25392378e3>:17] step = 10600, loss = 696175.125000\n",
      "[I 201229 00:02:23 <ipython-input-22-8f25392378e3>:19] 16.470 steps/sec\n",
      "[I 201229 00:02:29 <ipython-input-22-8f25392378e3>:17] step = 10700, loss = 1915799.500000\n",
      "[I 201229 00:02:29 <ipython-input-22-8f25392378e3>:19] 17.144 steps/sec\n",
      "[I 201229 00:02:36 <ipython-input-22-8f25392378e3>:17] step = 10800, loss = 1167846.625000\n",
      "[I 201229 00:02:36 <ipython-input-22-8f25392378e3>:19] 15.175 steps/sec\n",
      "[I 201229 00:02:42 <ipython-input-22-8f25392378e3>:17] step = 10900, loss = 918948.187500\n",
      "[I 201229 00:02:42 <ipython-input-22-8f25392378e3>:19] 16.194 steps/sec\n"
     ]
    }
   ],
   "source": [
    "while global_step_val < num_iterations:\n",
    "    start_time = time.time()\n",
    "    time_step, policy_state = collect_actor.run(\n",
    "        time_step=time_step,\n",
    "        policy_state=policy_state,\n",
    "    )\n",
    "    for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "    time_acc += time.time() - start_time\n",
    "\n",
    "    global_step_val = global_step.numpy()\n",
    "    if episode_val != train_metrics[0].result().numpy():\n",
    "        episode_val = train_metrics[0].result().numpy()\n",
    "        Server(14100).send_gui(\"DVAWrite RL_Agent.Episodes %d\" % episode_val)\n",
    "\n",
    "    if global_step_val % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step_val, train_loss.loss)\n",
    "        steps_per_sec = (global_step_val - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step_val\n",
    "        time_acc = 0\n",
    "\n",
    "    for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "    if global_step_val % eval_interval == 0:\n",
    "        Server(14100).send_gui(\"StopSim\")\n",
    "        time.sleep(0.5)\n",
    "        Server(14100).send_gui(\"StartSim\")\n",
    "        time.sleep(0.25)\n",
    "        Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 1)\n",
    "        time.sleep(0.25)\n",
    "        tf_eval_env.reset()\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            tf_eval_env,\n",
    "            tf_eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        tf_eval_env.reset()\n",
    "        time.sleep(0.5)\n",
    "        Server(14100).send_gui(\"DVAWrite RL_Agent.Signal %d\" % 0)\n",
    "\n",
    "    if eval_metrics_callback is not None:\n",
    "        eval_metrics_callback(results, global_step_val)\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    if global_step_val % train_checkpoint_interval == 0:\n",
    "        train_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "    if global_step_val % policy_checkpoint_interval == 0:\n",
    "        policy_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "    if global_step_val % rb_checkpoint_interval == 0:\n",
    "        rb_checkpointer.save(global_step=global_step_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = metric_utils.eager_compute(\n",
    "    eval_metrics,\n",
    "    tf_eval_env,\n",
    "    tf_eval_policy,\n",
    "    num_episodes=num_eval_episodes,\n",
    "    train_step=global_step,\n",
    "    summary_writer=eval_summary_writer,\n",
    "    summary_prefix='Metrics',\n",
    ")"
   ]
  }
 ]
}