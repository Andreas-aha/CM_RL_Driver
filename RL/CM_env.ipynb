{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bittfcudaconda14f52ee0a6104a05a960e9894e225f0d",
   "display_name": "Python 3.8.5 64-bit ('tf_cuda': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import time\n",
    "\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "#import reverb\n",
    "import tempfile\n",
    "\n",
    "import logzero\n",
    "from logzero import logger as logging\n",
    "\n",
    "from tf_agents.environments import py_environment, batched_py_environment, tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.experimental.train import actor\n",
    "from tf_agents.experimental.train import learner\n",
    "from tf_agents.experimental.train import triggers\n",
    "from tf_agents.experimental.train.utils import spec_utils\n",
    "from tf_agents.experimental.train.utils import strategy_utils\n",
    "from tf_agents.experimental.train.utils import train_utils\n",
    "from tf_agents.metrics import py_metrics, tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy, py_tf_policy\n",
    "from tf_agents.policies import random_py_policy, random_tf_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.eval import metric_utils\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n",
    "\n",
    "\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from CM_py_env import CarMakerEnv\n",
    "from server import Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 13152033329906518600\n, name: \"/device:XLA_CPU:0\"\ndevice_type: \"XLA_CPU\"\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 11968989727887212644\nphysical_device_desc: \"device: XLA_CPU device\"\n]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#root_dir = \"%s/rl_data/run%s\" % (os.getcwd(),  timestr)\n",
    "#os.mkdir(root_dir)\n",
    "root_dir = \"/media/vmroot/Daten/RL_Metrics/run20201214-194710\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter\n",
    "num_iterations = 4000000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
    "train_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000 # @param {type:\"integer\"}\n",
    "collect_actor_num_steps = 2\n",
    "collect_actor_num_episodes = None # steps or episodes must be none\n",
    "\n",
    "batch_size = 512 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 4e-4 # @param {typ\"}ber\"}\n",
    "actor_learning_rate = 8e-4 # @param {type\"}er\"}\n",
    "alpha_learning_rate = 4e-4 # @param {type\"}er\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 15.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 100 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 3 # @param {type:\"integer\"}\n",
    "eval_interval = 10000000000000000000 # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "summary_interval=1000\n",
    "summaries_flush_secs=10\n",
    "\n",
    "eval_metrics_callback=None\n",
    "\n",
    "train_checkpoint_interval=10000\n",
    "policy_checkpoint_interval=10000\n",
    "rb_checkpoint_interval=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logzero.logfile(\"%s/rotating-logfile.log\" % root_dir, maxBytes=1e6, backupCount=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(root_dir, 'train')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "train_summary_writer.set_as_default()\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "    eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Environment for collecting data during eval, and one for eval\n",
    "#Collect langsamer\n",
    "\n",
    "collect_env = CarMakerEnv(RTFac=1.7, mode='collect', gamma=gamma, server=0)\n",
    "#collect_env2 = CarMakerEnv(RTFac=2, mode='collect', gamma=gamma, server=1)\n",
    "#collect_env3 = CarMakerEnv(RTFac=1, mode='collect', gamma=gamma, server=2)\n",
    "#collect_env4 = CarMakerEnv(RTFac=1, mode='collect', gamma=gamma, server=3)\n",
    "\n",
    "#collect_env = batched_py_environment.BatchedPyEnvironment( [collect_env1, collect_env2] )\n",
    "#eval_env = CarMakerEnv(RTFac=999999, mode='evaluate', gamma=gamma, server=1)\n",
    "\n",
    "#eval_env = collect_env\n",
    "\n",
    "tf_collect_env = tf_py_environment.TFPyEnvironment(collect_env)\n",
    "tf_eval_env = tf_collect_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\nTensorSpec(shape=(18,), dtype=tf.float32, name='observation')\nAction Spec:\nBoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array([-1.  , -3.14], dtype=float32), maximum=array([1.  , 3.14], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Check specs\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(tf_collect_env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(tf_collect_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agents\n",
    "# All variables and Agents need to be created under strategy.scope(), as you'll see below.\n",
    "\n",
    "# Critic for estimate of action values\n",
    "# Input is observation and an action\n",
    "# Output is estimate of action value (to see how good it would be)\n",
    "\n",
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(tf_collect_env))\n",
    "\n",
    "\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "      (observation_spec, action_spec),\n",
    "      observation_fc_layer_params=None,\n",
    "      action_fc_layer_params=None,\n",
    "      joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "      kernel_initializer='glorot_uniform',\n",
    "      last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor network predicts parameters for a tanh-squashed MultivariateNormalDiag distribution.\n",
    "# Will be sampled conditioned on the current state (observation), if generation of action is needed.\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=actor_fc_layer_params,\n",
    "    continuous_projection_net=(\n",
    "        tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init agent\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "tf_agent = sac_agent.SacAgent(\n",
    "    time_step_spec,\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    train_step_counter=global_step)\n",
    "\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=replay_buffer_capacity)\n",
    "# 6 instead of 2 for TD(5)\n",
    "\n",
    "def _filter_invalid_transition(trajectories, unused_arg1):\n",
    "      return ~trajectories.is_boundary()[0]\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size, num_steps=2, num_parallel_calls=2).unbatch().filter(\n",
    "            _filter_invalid_transition).batch(batch_size).prefetch(100)\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policies\n",
    "# Agent has 2 policies\n",
    "\n",
    "# agent.policy — The main policy that is used for evaluation and deployment.\n",
    "# agent.collect_policy — A second policy that is used for data collection.\n",
    "\n",
    "tf_eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n",
    "\n",
    "tf_collect_policy = tf_agent.collect_policy\n",
    "\n",
    "# Additional random policy for initial collector\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "  tf_collect_env.time_step_spec(), tf_collect_env.action_spec())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_collect_env.batch_size),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_collect_env.batch_size),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8e703200d0>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=train_dir,\n",
    "    agent=tf_agent,\n",
    "    global_step=global_step,\n",
    "    metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "policy_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(train_dir, 'policy'),\n",
    "    policy=tf_eval_policy,\n",
    "    global_step=global_step)\n",
    "rb_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n",
    "    max_to_keep=1,\n",
    "    replay_buffer=replay_buffer)\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "rb_checkpointer.initialize_or_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor with the random policy and collect experiences to seed the replay buffer with\n",
    "\n",
    "initial_collect_actor = dynamic_step_driver.DynamicStepDriver(\n",
    "  tf_eval_env,\n",
    "  random_policy,\n",
    "  num_steps=initial_collect_steps,\n",
    "  observers=replay_observer+train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Actor with the collect policy to gather more experiences during training\n",
    "\n",
    "# Steps per run nun 5 statt 1, weil smoother?\n",
    "\n",
    "\n",
    "if collect_actor_num_episodes is not None and collect_actor_num_steps is not None:\n",
    "  raise ValueError(\"Define num episodes OR num steps. One of them must be None.\")\n",
    "elif collect_actor_num_episodes is None:\n",
    "  collect_actor = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_collect_env,\n",
    "    tf_collect_policy,\n",
    "    num_steps=collect_actor_num_steps,\n",
    "    observers=replay_observer+train_metrics)\n",
    "elif collect_actor_num_steps is None:\n",
    "  collect_actor = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_collect_env,\n",
    "    tf_collect_policy,\n",
    "    num_episodes=collect_actor_num_episodes,\n",
    "    observers=replay_observer+train_metrics)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_actor.run = common.function(initial_collect_actor.run)\n",
    "collect_actor.run = common.function(collect_actor.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = None\n",
    "policy_state = tf_collect_policy.get_initial_state(tf_collect_env.batch_size)\n",
    "timed_at_step = global_step.numpy()\n",
    "time_acc = 0\n",
    "global_episode_val = train_metrics[0].result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if replay_buffer.num_frames() == 0:\n",
    "    # Collect initial replay data.\n",
    "    print(\n",
    "        'Initializing replay buffer by collecting experience for %d steps '\n",
    "        'with a random policy.' % initial_collect_steps)\n",
    "    initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    experience, _ = next(iterator)\n",
    "    return tf_agent.train(experience)\n",
    "\n",
    "train_step = common.function(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step_val = global_step.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/drivers/dynamic_step_driver.py:196: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
      "WARNING:tensorflow:From /home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/drivers/dynamic_step_driver.py:196: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
      "WARNING:tensorflow:From /home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:159: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "WARNING:tensorflow:From /home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:159: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": " AttributeError: 'Server' object has no attribute 'old_msg_roh'\nTraceback (most recent call last):\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py\", line 318, in _isolated_step_py\n    return self._execute(_step_py, *flattened_actions)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py\", line 214, in _execute\n    return fn(*args, **kwargs)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py\", line 314, in _step_py\n    self._time_step = self._env.step(packed)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 203, in step\n    self._current_time_step = self._step(action)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/batched_py_environment.py\", line 166, in _step\n    time_steps = self._envs[0].step(actions)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 203, in step\n    self._current_time_step = self._step(action)\n\n  File \"/home/vmroot/CM_Projects/CM_RL_Driver/RL/CM_py_env.py\", line 146, in _step\n    self._state, self.sim_time, self.s_road = Server(self.tcp_port).server_step(action)\n\n  File \"/home/vmroot/CM_Projects/CM_RL_Driver/RL/server.py\", line 49, in server_step\n    message = self.old_msg_roh\n\nAttributeError: 'Server' object has no attribute 'old_msg_roh'\n\n\n\t [[{{node driver_loop/body/_1/driver_loop/step/step_py_func}}]] [Op:__inference_run_7383]\n\nFunction call stack:\nrun\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5f104a88b188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mglobal_step_val\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     time_step, policy_state = collect_actor.run(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  AttributeError: 'Server' object has no attribute 'old_msg_roh'\nTraceback (most recent call last):\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py\", line 318, in _isolated_step_py\n    return self._execute(_step_py, *flattened_actions)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py\", line 214, in _execute\n    return fn(*args, **kwargs)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/tf_py_environment.py\", line 314, in _step_py\n    self._time_step = self._env.step(packed)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 203, in step\n    self._current_time_step = self._step(action)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/batched_py_environment.py\", line 166, in _step\n    time_steps = self._envs[0].step(actions)\n\n  File \"/home/vmroot/miniconda3/envs/tf_cuda/lib/python3.8/site-packages/tf_agents/environments/py_environment.py\", line 203, in step\n    self._current_time_step = self._step(action)\n\n  File \"/home/vmroot/CM_Projects/CM_RL_Driver/RL/CM_py_env.py\", line 146, in _step\n    self._state, self.sim_time, self.s_road = Server(self.tcp_port).server_step(action)\n\n  File \"/home/vmroot/CM_Projects/CM_RL_Driver/RL/server.py\", line 49, in server_step\n    message = self.old_msg_roh\n\nAttributeError: 'Server' object has no attribute 'old_msg_roh'\n\n\n\t [[{{node driver_loop/body/_1/driver_loop/step/step_py_func}}]] [Op:__inference_run_7383]\n\nFunction call stack:\nrun\n"
     ]
    }
   ],
   "source": [
    "while global_step_val < num_iterations:\n",
    "    start_time = time.time()\n",
    "    time_step, policy_state = collect_actor.run(\n",
    "        time_step=time_step,\n",
    "        policy_state=policy_state,\n",
    "    )\n",
    "    Server(14100).server_nrt(str(global_episode_val))\n",
    "    for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "    time_acc += time.time() - start_time\n",
    "\n",
    "    global_step_val = global_step.numpy()\n",
    "    global_episode_val = train_metrics[0].result().numpy()\n",
    "\n",
    "\n",
    "    if global_step_val % log_interval == 0:\n",
    "        logging.info('step = %d, episode = %d, loss = %f', global_step_val, global_episode_val, train_loss.loss)\n",
    "        steps_per_sec = (global_step_val - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step_val\n",
    "        time_acc = 0\n",
    "\n",
    "    for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "    if global_step_val % eval_interval == 0:\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            tf_eval_env,\n",
    "            tf_eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "    if eval_metrics_callback is not None:\n",
    "        eval_metrics_callback(results, global_step_val)\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    if global_step_val % train_checkpoint_interval == 0:\n",
    "        train_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "    if global_step_val % policy_checkpoint_interval == 0:\n",
    "        policy_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "    if global_step_val % rb_checkpoint_interval == 0:\n",
    "        rb_checkpointer.save(global_step=global_step_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = metric_utils.eager_compute(\n",
    "    eval_metrics,\n",
    "    tf_eval_env,\n",
    "    tf_eval_policy,\n",
    "    num_episodes=num_eval_episodes,\n",
    "    train_step=global_step,\n",
    "    summary_writer=eval_summary_writer,\n",
    "    summary_prefix='Metrics',\n",
    ")"
   ]
  }
 ]
}